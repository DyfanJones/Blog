<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>paws on Dyfan Jones Brain Dump HQ</title>
    <link>/tags/paws/</link>
    <description>Recent content in paws on Dyfan Jones Brain Dump HQ</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-uk</language>
    <lastBuildDate>Sat, 22 Feb 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/tags/paws/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>The next package release into AWS Athena</title>
      <link>/post/the-next-package-release-into-aws-athena/</link>
      <pubDate>Sat, 22 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/the-next-package-release-into-aws-athena/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://www.r-bloggers.com&#34;&gt;RBloggers&lt;/a&gt;|&lt;a href=&#34;https://feeds.feedburner.com/RBloggers&#34;&gt;RBloggers-feedburner&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;RAthena 1.7.1&lt;/code&gt; and &lt;code&gt;noctua 1.5.1&lt;/code&gt; package versions have now been released to the CRAN. They both bring along several improvements with the connection to &lt;code&gt;AWS Athena&lt;/code&gt;, noticeably the performance speed and several creature comforts.&lt;/p&gt;

&lt;p&gt;These packages have both been designed to reflect one another,even down to how they connect to &lt;code&gt;AWS Athena&lt;/code&gt;. This means that all features going forward will exist in both packages. I will refer to these packages as one, as they basically work in the same way.&lt;/p&gt;

&lt;h1 id=&#34;performance-improvements&#34;&gt;Performance improvements:&lt;/h1&gt;

&lt;p&gt;Initially the packages utilised &lt;code&gt;AWS Athena&lt;/code&gt; SQL queries. This was to achieve all the functional requirements of the &lt;code&gt;DBI&lt;/code&gt; package framework. However the package would always send a SQL query to &lt;code&gt;AWS Athena&lt;/code&gt; which in turn would have to lift a flat file from &lt;code&gt;AWS S3&lt;/code&gt;, before returning the final result to &lt;code&gt;R&lt;/code&gt;. This means the performance of the packages would be limited and fairly slow compared to other data base backends.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/2020-02-22-the-next-package-release-into-aws-athena_files/r-athena-old.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The biggest change is the adoption of more functionality of the SDKs (software development kit) into AWS. The key component that has been adopted is &lt;code&gt;AWS Glue&lt;/code&gt;. &lt;code&gt;AWS Glue&lt;/code&gt; contains all of &lt;code&gt;AWS Athena&lt;/code&gt; table DDL&amp;rsquo;s. This means instead of going to &lt;code&gt;AWS Athena&lt;/code&gt; for this information &lt;code&gt;AWS Glue&lt;/code&gt; can be used instead.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/2020-02-22-the-next-package-release-into-aws-athena_files/r-athena-new.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;By utilising &lt;code&gt;AWS Glue&lt;/code&gt;, the table meta data (column names, column types, schema hierarchy etc&amp;hellip;) can easily be retrieved at a fraction of the time it would of taken to query &lt;code&gt;AWS Athena&lt;/code&gt;. Previously the &lt;code&gt;DBI&lt;/code&gt; function &lt;code&gt;dbListTables&lt;/code&gt; would send a query to &lt;code&gt;AWS Athena&lt;/code&gt;, this would retrieve all the tables listed in all schemas. This would take over 3 seconds. Now using &lt;code&gt;AWS Glue&lt;/code&gt; to retrieve the same data, it takes less than 0.5 of a second.&lt;/p&gt;

&lt;h2 id=&#34;dplyr&#34;&gt;dplyr&lt;/h2&gt;

&lt;p&gt;When &lt;code&gt;AWS Glue&lt;/code&gt; is used to collect metadata around a table in &lt;code&gt;AWS Athena&lt;/code&gt;, a performance in &lt;code&gt;dplyr::tbl&lt;/code&gt; can be done. I would like to say thanks to @OssiLehtinen for developing the initial implementation as this improvement would have been overlooked.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;dplyr::tbl&lt;/code&gt; has two key methods when creating the initial object. The first is called SQL identifiers and this is the method that benefits from the new &lt;code&gt;AWS Glue&lt;/code&gt; functionality. To use SQL identifiers is fairly straight forward.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(DBI)
library(dplyr)
library(RAthena) #Or library(noctua)

con = dbConnect(athena())

dbWriteTable(con, &amp;quot;iris&amp;quot;, iris)

ident_iris = tbl(con, &amp;quot;iris&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;dplyr&lt;/code&gt; can identify the &lt;code&gt;iris&lt;/code&gt; table within the connected schema. When a user uses the SQL identifier method in &lt;code&gt;dplyr::tbl&lt;/code&gt;, &lt;code&gt;AWS Glue&lt;/code&gt; is called to retrieve all the meta data for &lt;code&gt;dplyr&lt;/code&gt;. This increases the performance from 3.66 to 0.29 seconds. The second method is called SQL sub query. This unfortunately won&amp;rsquo;t benefit from the new feature and will run in slower at 3.66 seconds.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;subquery_iris = tbl(con, sql(&amp;quot;select * from iris&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Therefore I recommend the use of SQL identifier method when using &lt;code&gt;dplyr&#39;s&lt;/code&gt; interface.&lt;/p&gt;

&lt;h1 id=&#34;creature-comforts&#34;&gt;Creature Comforts&lt;/h1&gt;

&lt;h2 id=&#34;aws-athena-metadata&#34;&gt;AWS Athena Metadata&lt;/h2&gt;

&lt;p&gt;Due to user feature requests the packages now return more meta data around each query sent to &lt;code&gt;AWS Athena&lt;/code&gt;. Thus the basic level of meta data returned, is the amount of data scanned by &lt;code&gt;AWS Athena&lt;/code&gt;. This is formatted into a readable format depending on the amount of data scanned.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(DBI)
library(RAthena) #Or library(noctua)

con = dbConnect(athena())

dbWriteTable(con, &amp;quot;iris&amp;quot;, iris)

dbGetQuery(con, &amp;quot;select * from iris&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Info: (Data scanned: 860 Bytes)
     sepal_length sepal_width petal_length petal_width   species
  1:          5.1         3.5          1.4         0.2    setosa
  2:          4.9         3.0          1.4         0.2    setosa
  3:          4.7         3.2          1.3         0.2    setosa
  4:          4.6         3.1          1.5         0.2    setosa
  5:          5.0         3.6          1.4         0.2    setosa
 ---                                                            
146:          6.7         3.0          5.2         2.3 virginica
147:          6.3         2.5          5.0         1.9 virginica
148:          6.5         3.0          5.2         2.0 virginica
149:          6.2         3.4          5.4         2.3 virginica
150:          5.9         3.0          5.1         1.8 virginica
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However if you set the new parameter &lt;code&gt;statistics&lt;/code&gt; to &lt;code&gt;TRUE&lt;/code&gt; then all the metadata around that query is printed out like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dbGetQuery(con, &amp;quot;select * from iris&amp;quot;, statistics = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$EngineExecutionTimeInMillis
[1] 1568

$DataScannedInBytes
[1] 860

$DataManifestLocation
character(0)

$TotalExecutionTimeInMillis
[1] 1794

$QueryQueueTimeInMillis
[1] 209

$QueryPlanningTimeInMillis
[1] 877

$ServiceProcessingTimeInMillis
[1] 17

Info: (Data scanned: 860 Bytes)
     sepal_length sepal_width petal_length petal_width   species
  1:          5.1         3.5          1.4         0.2    setosa
  2:          4.9         3.0          1.4         0.2    setosa
  3:          4.7         3.2          1.3         0.2    setosa
  4:          4.6         3.1          1.5         0.2    setosa
  5:          5.0         3.6          1.4         0.2    setosa
 ---                                                            
146:          6.7         3.0          5.2         2.3 virginica
147:          6.3         2.5          5.0         1.9 virginica
148:          6.5         3.0          5.2         2.0 virginica
149:          6.2         3.4          5.4         2.3 virginica
150:          5.9         3.0          5.1         1.8 virginica
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can also be retrieved by using &lt;code&gt;dbStatistics&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;res = dbExecute(con, &amp;quot;select * from iris&amp;quot;)

# return query statistic
query_stats = dbStatistics(res)

# return query results
dbFetch(res)

# Free all resources
dbClearResult(res)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;rjdbc-inspired-function&#34;&gt;&lt;code&gt;RJDBC&lt;/code&gt; inspired function&lt;/h2&gt;

&lt;p&gt;I have to give full credit to the package &lt;code&gt;RJDBC&lt;/code&gt; for inspiring me to create this function. &lt;code&gt;DBI&lt;/code&gt; has got a good function called &lt;code&gt;dbListTables&lt;/code&gt; that will list all the tables that are in &lt;code&gt;AWS Athena&lt;/code&gt;. However it won&amp;rsquo;t return to which schema each individual table is related to. To over come this &lt;code&gt;RJDBC&lt;/code&gt; has a excellent function called &lt;code&gt;dbGetTables&lt;/code&gt;. This function returns all the tables from &lt;code&gt;AWS Athena&lt;/code&gt; as a &lt;code&gt;data.frame&lt;/code&gt;. This has the advantage of detailing schema, table and table type. With the new integration into &lt;code&gt;AWS Glue&lt;/code&gt; this can be returned quickly.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dbGetTables(con)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;      Schema             TableName      TableType
 1:  default             df_bigint EXTERNAL_TABLE
 2:  default                  iris EXTERNAL_TABLE
 3:  default               mtcars2 EXTERNAL_TABLE
 4:  default         nyc_taxi_2018 EXTERNAL_TABLE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This just makes it a little bit easier when working in different IDE&amp;rsquo;s for example &lt;code&gt;Jupyter&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;backend-option-changes&#34;&gt;Backend option changes&lt;/h2&gt;

&lt;p&gt;This is not really a creature comfort but it still interesting and useful. Both packages are dependent on &lt;code&gt;data.table&lt;/code&gt; to read data into &lt;code&gt;R&lt;/code&gt;. This is down to the amazing speed &lt;code&gt;data.table&lt;/code&gt; offers when reading files into &lt;code&gt;R&lt;/code&gt;. However a new package, with equally impressive read speeds, has come onto the scene called &lt;a href=&#34;https://github.com/r-lib/vroom&#34;&gt;&lt;code&gt;vroom&lt;/code&gt;&lt;/a&gt;. As &lt;code&gt;vroom&lt;/code&gt; has been designed to only read data into &lt;code&gt;R&lt;/code&gt; similarly to &lt;code&gt;readr&lt;/code&gt;, &lt;code&gt;data.table&lt;/code&gt; is still used for all of the heavy lifting. However if a user wishes to use &lt;code&gt;vroom&lt;/code&gt; as the file parser an &lt;code&gt;*_options&lt;/code&gt; function has been created to enable this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nocuta_options(file_parser = c(&amp;quot;data.table&amp;quot;, &amp;quot;vroom&amp;quot;))

# Or 

RAthena__options(file_parser = c(&amp;quot;data.table&amp;quot;, &amp;quot;vroom&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By setting the file_parser to &lt;code&gt;vroom&lt;/code&gt; then the backend will change to allow &lt;code&gt;vroom&#39;s&lt;/code&gt; file parser to be used instead of &lt;code&gt;data.table&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you aren&amp;rsquo;t sure whether to use &lt;code&gt;vroom&lt;/code&gt; over &lt;code&gt;data.table&lt;/code&gt;, I draw your attention to &lt;code&gt;vroom&lt;/code&gt; boasting a whopping 1.40GB/sec throughput.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Statistics taken from vroom&amp;rsquo;s github readme&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;package&lt;/th&gt;
&lt;th&gt;version&lt;/th&gt;
&lt;th&gt;time (sec)&lt;/th&gt;
&lt;th&gt;speed-up&lt;/th&gt;
&lt;th&gt;throughput&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;vroom&lt;/td&gt;
&lt;td&gt;1.1.0&lt;/td&gt;
&lt;td&gt;1.14&lt;/td&gt;
&lt;td&gt;58.44&lt;/td&gt;
&lt;td&gt;1.40 GB/sec&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;data.table&lt;/td&gt;
&lt;td&gt;1.12.8&lt;/td&gt;
&lt;td&gt;11.88&lt;/td&gt;
&lt;td&gt;5.62&lt;/td&gt;
&lt;td&gt;134.13 MB/sec&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;readr&lt;/td&gt;
&lt;td&gt;1.3.1&lt;/td&gt;
&lt;td&gt;29.02&lt;/td&gt;
&lt;td&gt;2.30&lt;/td&gt;
&lt;td&gt;54.92 MB/sec&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;read.delim&lt;/td&gt;
&lt;td&gt;3.6.2&lt;/td&gt;
&lt;td&gt;66.74&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;23.88 MB/sec&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;rstudio-interface&#34;&gt;RStudio Interface!&lt;/h2&gt;

&lt;p&gt;Due to the ability of &lt;code&gt;AWS Glue&lt;/code&gt; to retrieve metadata for &lt;code&gt;AWS Athena&lt;/code&gt; at speed, it has now been possible to add the interface into RStudio&amp;rsquo;s connection tab. When a connection is established:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(DBI)
library(RAthena) #Or library(noctua)

con = dbConnect(athena())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The connection icon will as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/2020-02-22-the-next-package-release-into-aws-athena_files/rstudio-con-tab.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The AWS region you are connecting to will be reflected in the connection (highlighted above in the red square). This is to help users that are able to connect to multiple different &lt;code&gt;AWS Athena&lt;/code&gt; over different regions.&lt;/p&gt;

&lt;p&gt;Once you have connected &lt;code&gt;AWS Athena&lt;/code&gt;, schema hierarchy will be displayed. In my example you can see some of the tables I have created when testing these packages.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/2020-02-22-the-next-package-release-into-aws-athena_files/rstudio-con-schema.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For more information around RStudio&amp;rsquo;s connection tab please check out &lt;a href=&#34;https://blog.rstudio.com/2017/08/16/rstudio-preview-connections/&#34;&gt;RStudio preview connections&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;

&lt;p&gt;To sum up, the &lt;code&gt;Rathena&lt;/code&gt; and &lt;code&gt;noctua&lt;/code&gt; latest versions have been released to cran with all the new goodies they bring. As these packages are based on AWS SDK&amp;rsquo;s they are highly customisable. Features can easily be added to improve the packages when connecting to &lt;code&gt;AWS Athena&lt;/code&gt;. So please raise any feature requests / bug issues to: &lt;a href=&#34;https://github.com/DyfanJones/RAthena&#34;&gt;https://github.com/DyfanJones/RAthena&lt;/a&gt; and &lt;a href=&#34;https://github.com/DyfanJones/noctua&#34;&gt;https://github.com/DyfanJones/noctua&lt;/a&gt;&lt;/p&gt;
</description>
      
                  <category>R</category>
      
                  <category>Athena</category>
      
                  <category>Boto3</category>
      
                  <category>paws</category>
      
                  <category>Python</category>
      
      
            <category>RBloggers</category>
      
    </item>
    
    <item>
      <title>R Owl of Athena</title>
      <link>/post/r-owl-of-athena/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/r-owl-of-athena/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://www.r-bloggers.com&#34;&gt;RBloggers&lt;/a&gt;|&lt;a href=&#34;https://feeds.feedburner.com/RBloggers&#34;&gt;RBloggers-feedburner&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;intro&#34;&gt;Intro:&lt;/h1&gt;

&lt;p&gt;After developing the package &lt;a href=&#34;https://cran.r-project.org/web/packages/RAthena/index.html&#34;&gt;&lt;code&gt;RAthena&lt;/code&gt;&lt;/a&gt;, I stumbled quite accidentally into the R SDK for AWS &lt;a href=&#34;https://github.com/paws-r/paws&#34;&gt;&lt;code&gt;paws&lt;/code&gt;&lt;/a&gt;. As &lt;code&gt;RAthena&lt;/code&gt; utilises Python&amp;rsquo;s SDK &lt;a href=&#34;https://boto3.amazonaws.com/v1/documentation/api/latest/index.html?id=docs_gateway&#34;&gt;&lt;code&gt;boto3&lt;/code&gt;&lt;/a&gt; I thought the development of another AWS Athena package couldn&amp;rsquo;t hurt. As mentioned in my &lt;a href=&#34;https://dyfanjones.me/post/an-amazon-sdk-for-r/&#34;&gt;previous blog&lt;/a&gt; the &lt;code&gt;paws&lt;/code&gt; syntax is very similar to &lt;code&gt;boto3&lt;/code&gt; so alot of my &lt;code&gt;RAthena&lt;/code&gt; code was very portable and this gave me my final excuse to develop my next R package.&lt;/p&gt;

&lt;h1 id=&#34;paws-and-aws-athena&#34;&gt;&lt;code&gt;paws&lt;/code&gt; and AWS Athena:&lt;/h1&gt;

&lt;p&gt;Before getting into the next package, lets first look at how the SDK&amp;rsquo;s interact with AWS Athena.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;For example: return all databases in AWS Athena&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;# create an AWS Athena object
athena &amp;lt;- paws::athena()

# Submit query to AWS Athena
res &amp;lt;- athena$start_query_execution(
            QueryString = &amp;quot;show Databases&amp;quot;,
            ResultConfiguration = 
                list(OutputLocation = &amp;quot;s3://mybucket/queries/&amp;quot;))

# Get Status of query
result &amp;lt;- athena$get_query_execution(QueryExecutionId = res$QueryExecutionId)

# Return results if query is successful
if(result$QueryExecution$Status$State == &amp;quot;FAILED&amp;quot;) {
  stop(result$QueryExecution$Status$StateChangeReason, call. = FALSE)
} else {output &amp;lt;- 
          athena$get_query_results(
              QueryExecutionId = res$QueryExecutionId,
              MaxResults = 10)}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This isn&amp;rsquo;t the prettiest code when wanting to query AWS Athena with the SQL, in the above example: &lt;code&gt;SHOW DATABASES&lt;/code&gt;. This example only returns the top 10 results. It is even more &amp;ldquo;interesting&amp;rdquo; if you wish to return the entire data frame from AWS Athena. This is where &lt;code&gt;noctua&lt;/code&gt; comes in.&lt;/p&gt;

&lt;h1 id=&#34;noctua&#34;&gt;&lt;code&gt;noctua&lt;/code&gt;&lt;/h1&gt;

&lt;p&gt;To start off with I will go through the same 3 questions I went through in my &lt;a href=&#34;https://dyfanjones.me/post/athena-and-r-there-is-another-way/&#34;&gt;Athena and R &amp;hellip; there is another way!?&lt;/a&gt; blog.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;What is noctua?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;noctua&lt;/code&gt; is a R package that creates a &lt;code&gt;DBI&lt;/code&gt; (Database Interface) for R, using the R package &lt;code&gt;DBI&lt;/code&gt; and the R SDK &lt;code&gt;paws&lt;/code&gt; as the backend (so basically the same as &lt;code&gt;RAthena&lt;/code&gt;)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Why was &lt;code&gt;noctua&lt;/code&gt; created when there are already methods for connecting to Athena?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;noctua&lt;/code&gt; was created to provide an extra method to connect to Athena for R users. Plus it seemed natural to create &lt;code&gt;noctua&lt;/code&gt; due to the nature in how it connects to AWS Athena (through a SDK), which is the method &lt;code&gt;RAthena&lt;/code&gt; connects to AWS Athena.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Why is &lt;code&gt;noctua&lt;/code&gt; called &lt;code&gt;noctua&lt;/code&gt;?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;This is a tricky one as &lt;code&gt;RAthena&lt;/code&gt; was already taken. So I looked for a historic reference to link the new package to AWS Athena. I settled on &lt;code&gt;noctua&lt;/code&gt; due to: &lt;a href=&#34;https://en.wikipedia.org/wiki/Athena&#34;&gt;Athena/Minerva&lt;/a&gt; is the Greek/Roman god of wisdom, handicraft, and warfare. One of the main symbols for Athena is the Owl. Noctua is the latin word for Owl.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;how-to-install&#34;&gt;How to install:&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;noctua&lt;/code&gt; is currently on the CRAN and Github:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;CRAN version:&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;noctua&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Github development version:&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;remotes::install_github(&amp;quot;dyfanjones/noctua&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;usage&#34;&gt;Usage:&lt;/h1&gt;

&lt;p&gt;As with all &lt;code&gt;DBI&lt;/code&gt; interface packages the key functions are exactly the same. Which means that there is little to no upskilling required. The only difference between each method is how they connect and send data back to the database. So we will focus mainly on those two aspects.&lt;/p&gt;

&lt;h2 id=&#34;connecting-to-aws-athena&#34;&gt;Connecting to AWS Athena:&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;noctua&lt;/code&gt; offers a wide range of connection methods from hard coding to using Amazon Resource Name Roles (ARN roles). Which is very similar to the &lt;code&gt;RAthena&lt;/code&gt; package.&lt;/p&gt;

&lt;h3 id=&#34;hard-coding-method&#34;&gt;Hard-Coding Method:&lt;/h3&gt;

&lt;p&gt;This method isn&amp;rsquo;t recommended as your credentials are hard-coded.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(DBI)
con &amp;lt;- dbConnect(noctua::athena(),
                 aws_access_key_id = &amp;quot;YOUR AWS KEY ID&amp;quot;,
                 aws_secret_access_key = &amp;quot;YOUR SECRET ACCESS KEY&amp;quot;,
                 s3_staging_dir = &amp;quot;s3://path/to/query/bucket/&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;em&gt;&lt;code&gt;s3_staging_dir&lt;/code&gt; requires to be in the format of &lt;code&gt;s3 uri&lt;/code&gt; for example &amp;ldquo;s3://path/to/query/bucket/&amp;rdquo;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If you do not wish to create AWS Profiles then setting environmental variables would be the recommended method.&lt;/p&gt;

&lt;h3 id=&#34;environment-variable-method&#34;&gt;Environment Variable Method:&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;noctua&lt;/code&gt; supports &lt;a href=&#34;https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html&#34;&gt;AWS credentials&lt;/a&gt; when set into the environment variables to avoid hard-coding. From what I have found out, an easy way to set up environment variables (that persists) in R is to use the &lt;code&gt;file.edit&lt;/code&gt; function like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;file.edit(&amp;quot;~/.Renviron&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now you can simply add in your environment variables in the file you are editing for example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;AWS_ACCESS_KEY_ID = YOUR AWS KEY ID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once you have set your environment variables you can connect to Athena in the following method:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(DBI)
con &amp;lt;- dbConnect(noctua::athena(),
                 s3_staging_dir = &amp;quot;s3://path/to/query/bucket/&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can set the &lt;code&gt;s3_staging_dir&lt;/code&gt; parameter as an environmental variable, to do this you need to set the following environmental variable:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;AWS_ATHENA_S3_STAGING_DIR = s3://path/to/query/bucket/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This allows for the following connection:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(DBI)
con &amp;lt;- dbConnect(noctua::athena())
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;aws-profile-names&#34;&gt;AWS Profile Names:&lt;/h3&gt;

&lt;p&gt;Another method is to use AWS Profile Names. AWS profile names can be setup either manually in the &lt;code&gt;~/.aws&lt;/code&gt; directory or by using the &lt;a href=&#34;https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html&#34;&gt;AWS Command Line Interface (AWS CLI)&lt;/a&gt;. Once you have setup your profile name you can connect to AWS Athena:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Using Default Profile Name:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(DBI)
con &amp;lt;- dbConnect(noctua::athena())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Using Non-Default Profile Name:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(DBI)
con &amp;lt;- dbConnect(noctua::athena(),
                 profile_name = &amp;quot;rathena&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;arn-roles&#34;&gt;ARN Roles:&lt;/h3&gt;

&lt;p&gt;ARN roles are fairly useful if you need to assume a role that can connect to another AWS account and use the AWS Athena in that account. Or whether you want to create a temporary connection with different permissions than your current role (&lt;a href=&#34;https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html&#34;&gt;AWS ARN Documentation&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Assuming ARN role credentials before connecting to AWS Athena:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(noctua)
library(DBI)
assume_role(profile_name = &amp;quot;YOUR_PROFILE_NAME&amp;quot;,
            role_arn = &amp;quot;arn:aws:sts::123456789012:assumed-role/role_name/role_session_name&amp;quot;,
            set_env = TRUE)

# Connect to Athena using ARN Role
con &amp;lt;- dbConnect(athena(),
                s3_staging_dir = &amp;quot;s3://path/to/query/bucket/&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Connect to AWS Athena directly using ARN role:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(DBI)
con &amp;lt;- dbConnect(noctua::athena(),
                  profile_name = &amp;quot;YOUR_PROFILE_NAME&amp;quot;,
                  role_arn = &amp;quot;arn:aws:sts::123456789012:assumed-role/role_name/role_session_name&amp;quot;,
                  s3_staging_dir = &#39;s3://path/to/query/bucket/&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;em&gt;ARN Roles have a duration timer before they will expire. To change the default you can increase the &lt;code&gt;duration_seconds&lt;/code&gt; parameter from the default 3600 seconds (1 hour).&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;temporary-sessions&#34;&gt;Temporary Sessions:&lt;/h3&gt;

&lt;p&gt;Finally you can create temporary credentials before connecting to AWS Athena:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(noctua)
library(DBI)

# Create Temporary Credentials duration 1 hour
get_session_token(&amp;quot;YOUR_PROFILE_NAME&amp;quot;,
                  serial_number=&#39;arn:aws:iam::123456789012:mfa/user&#39;,
                  token_code = &amp;quot;531602&amp;quot;,
                  set_env = TRUE)

# Connect to Athena using temporary credentials
con &amp;lt;- dbConnect(athena(),
                s3_staging_dir = &amp;quot;s3://path/to/query/bucket/&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;em&gt;This method will work for users who have set up &lt;a href=&#34;https://aws.amazon.com/iam/details/mfa/&#34;&gt;Multi-Factor Authentication&lt;/a&gt; (MFA).&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;querying&#34;&gt;Querying:&lt;/h2&gt;

&lt;p&gt;To query AWS Athena using the &lt;code&gt;noctua&lt;/code&gt; it is very similar to querying any other &lt;code&gt;DBI&lt;/code&gt; database method:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(DBI)

con &amp;lt;- dbConnect(noctua::athena())

dbGetQuery(con, &amp;quot;show databases&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That is it! So if we look back at the initial &lt;code&gt;paws&lt;/code&gt; code when working with AWS Athena. The code was very intimidating when wanting to do basic AWS Athena queries. &lt;code&gt;noctua&lt;/code&gt; packages all that up and makes it super easy to work with.&lt;/p&gt;

&lt;h2 id=&#34;uploading-data&#34;&gt;Uploading Data:&lt;/h2&gt;

&lt;p&gt;It is all very well querying data from AWS Athena but what is more useful is to upload data as well. &lt;code&gt;noctua&lt;/code&gt; has addressed this and implemented a method in &lt;code&gt;dbWriteTable&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dbWriteTable(con, &amp;quot;mtcars&amp;quot;, mtcars,
             partition=c(&amp;quot;TIMESTAMP&amp;quot; = format(Sys.Date(), &amp;quot;%Y%m%d&amp;quot;)),
             s3.location = &amp;quot;s3://mybucket/data/&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once you have uploaded data into &lt;code&gt;AWS Athena&lt;/code&gt; you can query it in the following:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dbGetQuery(con, &amp;quot;select * from mtcars&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here are all variable parameters for the &lt;code&gt;dbWriteTable&lt;/code&gt; method:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;conn:&lt;/strong&gt; An AthenaConnection object, produced by dbConnect()&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;name:&lt;/strong&gt; A character string specifying a table name. Names will be automatically quoted so you can use any sequence of characters, not just any valid bare table name.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;value:&lt;/strong&gt; A data.frame to write to the database.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;overwrite:&lt;/strong&gt; Allow overwriting the destination table. Cannot be &amp;lsquo;TRUE&amp;rsquo; if &amp;lsquo;append&amp;rsquo; is also &amp;lsquo;TRUE&amp;rsquo;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;append:&lt;/strong&gt; Allow appending to the destination table. Cannot be &amp;lsquo;TRUE&amp;rsquo; if &amp;lsquo;overwrite&amp;rsquo; is also &amp;lsquo;TRUE&amp;rsquo;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;row.names:&lt;/strong&gt; Either TRUE, FALSE, NA or a string. If TRUE, always translate row names to a column called &amp;ldquo;row_names&amp;rdquo;. If FALSE, never translate row names. If NA, translate rownames only if they&amp;rsquo;re a character vector. A string is equivalent to TRUE, but allows you to override the default name. For backward compatibility, NULL is equivalent to FALSE.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;field.types:&lt;/strong&gt; Additional field types used to override derived types.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;partition:&lt;/strong&gt; Partition Athena table (needs to be a named list or vector) for example: c(var1 = &amp;ldquo;2019-20-13&amp;rdquo;)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;s3.location&lt;/strong&gt; s3 bucket to store Athena table, must be set as a s3 uri for example (&amp;ldquo;s3://mybucket/data/&amp;ldquo;)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;file.type:&lt;/strong&gt; What file type to store data.frame on s3, RAthena currently supports [&amp;ldquo;csv&amp;rdquo;, &amp;ldquo;tsv&amp;rdquo;, &amp;ldquo;parquet&amp;rdquo;]. &lt;strong&gt;Note:&lt;/strong&gt; &lt;em&gt;file.type &amp;ldquo;parquet&amp;rdquo; is supported by R package &lt;a href=&#34;https://github.com/apache/arrow/tree/master/r&#34;&gt;&lt;code&gt;arrow&lt;/code&gt;&lt;/a&gt; and will need to be installed separately if you wish to upload data.frames in &amp;ldquo;parquet&amp;rdquo; format.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&amp;hellip;:&lt;/strong&gt; Other arguments used by individual methods.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion:&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;noctua&lt;/code&gt; is a package that gives R users the access to AWS Athena using the R AWS SDK &lt;code&gt;paws&lt;/code&gt;. Thus no external software is required and it can all be installed from the CRAN. If you are interested in how to connect R to AWS Athena please check out &lt;a href=&#34;https://cran.r-project.org/web/packages/RAthena/index.html&#34;&gt;&lt;code&gt;RAthena&lt;/code&gt;&lt;/a&gt; as well (my other AWS Athena connectivity R package). All feature requests/ suggestions/issues are welcome please add them to: &lt;a href=&#34;https://github.com/DyfanJones/noctua/issues&#34;&gt;Github Issues&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Finally please star the github repositories if you like the work that has been done with R and AWS Athena &lt;a href=&#34;https://github.com/DyfanJones/noctua&#34;&gt;&lt;code&gt;noctua&lt;/code&gt;&lt;/a&gt; , &lt;a href=&#34;https://github.com/DyfanJones/RAthena&#34;&gt;&lt;code&gt;RAthena&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
</description>
      
                  <category>R</category>
      
                  <category>paws</category>
      
                  <category>Athena</category>
      
      
            <category>RBloggers</category>
      
    </item>
    
    <item>
      <title>An Amazon SDK for R!?</title>
      <link>/post/an-amazon-sdk-for-r/</link>
      <pubDate>Tue, 29 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/an-amazon-sdk-for-r/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://www.r-bloggers.com&#34;&gt;RBloggers&lt;/a&gt;|&lt;a href=&#34;https://feeds.feedburner.com/RBloggers&#34;&gt;RBloggers-feedburner&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;intro&#34;&gt;Intro:&lt;/h1&gt;

&lt;p&gt;For a long time I have found it difficult to appreciate the benefits of &amp;ldquo;cloud compute&amp;rdquo; in my R model builds. This was due to my initial lack of understanding and the setting up of R on cloud compute environments. When I noticed that AWS was bringing out a new product &lt;a href=&#34;https://aws.amazon.com/sagemaker/&#34;&gt;AWS Sagemaker&lt;/a&gt;, the possiblities of what it could provide seemed like a dream come true.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Amazon SageMaker provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly. Amazon SageMaker is a fully-managed service that covers the entire machine learning workflow to label and prepare your data, choose an algorithm, train the model, tune and optimize it for deployment, make predictions, and take action. Your models get to production faster with much less effort and lower cost. (&lt;a href=&#34;https://aws.amazon.com/sagemaker/&#34;&gt;https://aws.amazon.com/sagemaker/&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A question about AWS Sagemake came to mind: &lt;em&gt;Does it work for R developers???&lt;/em&gt; Well&amp;hellip;not exactly. True it provides a simple way to set up an R environment in the cloud but it doesn&amp;rsquo;t give the means to access other AWS products for example &lt;a href=&#34;https://aws.amazon.com/s3/&#34;&gt;AWS S3&lt;/a&gt; and &lt;a href=&#34;https://aws.amazon.com/athena/&#34;&gt;AWS Athena&lt;/a&gt; out of the box. However for Python this is not a problem. Amazon has provided a Software Development Kit (SDK) for Python called &lt;a href=&#34;https://boto3.amazonaws.com/v1/documentation/api/latest/index.html&#34;&gt;&lt;code&gt;boto3&lt;/code&gt;&lt;/a&gt;, which comes pre-installed on AWS Sagemaker.&lt;/p&gt;

&lt;p&gt;It isn&amp;rsquo;t all bad news, RStudio has developed a package called &lt;a href=&#34;https://rstudio.github.io/reticulate/&#34;&gt;&lt;code&gt;reticulate&lt;/code&gt;&lt;/a&gt; that lets R interfaced into Python. So using &lt;code&gt;reticulate&lt;/code&gt; in combination with &lt;code&gt;boto3&lt;/code&gt; gives R full access to all of AWS products from Sagemaker similar to Python. However are there any other methods for R user to connect to AWS?&lt;/p&gt;

&lt;h1 id=&#34;aws-interfaces-for-r&#34;&gt;AWS interfaces for R:&lt;/h1&gt;

&lt;h2 id=&#34;paws-https-paws-r-github-io-an-r-sdk&#34;&gt;&lt;a href=&#34;https://paws-r.github.io/&#34;&gt;&lt;code&gt;paws&lt;/code&gt;&lt;/a&gt; an R SDK:&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Paws is a Package for Amazon Web Services in R. Paws provides access to the full suite of AWS services from within R.(&lt;a href=&#34;https://github.com/paws-r/paws&#34;&gt;https://github.com/paws-r/paws&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;When I want to connect to AWS I usually turn to Python. AWS&amp;rsquo;s &lt;code&gt;boto3&lt;/code&gt; is an excellent means of connecting to AWS and exploit its resources. However R now has it&amp;rsquo;s own SDK into AWS, &lt;code&gt;paws&lt;/code&gt;. This came as a little surprise to me as I started to accept that R might never have an SDK for AWS. How wrong I was.&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s pleasing to me was how well developed and easy the package was to use. It felt natural to switch between &lt;code&gt;boto3&lt;/code&gt; and &lt;code&gt;paws&lt;/code&gt;. Almost like it was a long lost brother.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Here is a quick example to show the comparison between &lt;code&gt;boto3&lt;/code&gt; and &lt;code&gt;paws&lt;/code&gt;. Returning a list of all objects in S3 inside a prefix:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Python&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import boto3

s3 = boto3.Session().client(&amp;quot;s3&amp;quot;)
obj = s3.list_objects(Bucket = &#39;mybucket&#39;, Prefix = &amp;quot;prefix_1/&amp;quot;)
[x.get(&amp;quot;Key&amp;quot;) for x in obj.get(&amp;quot;Contents&amp;quot;)]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;s3 &amp;lt;- paws::s3()

obj &amp;lt;- s3$list_objects(Bucket = &#39;mybucket&#39;, Prefix = &amp;quot;prefix_1/&amp;quot;)
lapply(obj$Contents, function(x) x$Key)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From this quick example it is clear that the &lt;code&gt;paws&lt;/code&gt; SDK&amp;rsquo;s syntax is extremely similar to &lt;code&gt;boto3&lt;/code&gt;, although with an R twist. This can only a good thing, as hundreds of people know &lt;code&gt;boto3&lt;/code&gt; already and therefore they will be familiar with &lt;code&gt;paws&lt;/code&gt; by association. I can&amp;rsquo;t express the potential the package &lt;code&gt;paws&lt;/code&gt; gives R users. A good project that utilises the &lt;code&gt;paws&lt;/code&gt; sdk is the package &lt;a href=&#34;https://cran.r-project.org/web/packages/noctua/index.html&#34;&gt;&lt;code&gt;noctua&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;noctua&lt;/code&gt; creates a wrapper of the &lt;code&gt;paws&lt;/code&gt; connection to AWS Athena and developes a &lt;code&gt;DBI&lt;/code&gt; interface for R users. We will go into the package &lt;code&gt;noctua&lt;/code&gt; in the next blog. First here is an example how of to work with AWS Athena when using &lt;code&gt;paws&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Querying to AWS Athena using &lt;code&gt;paws&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# create an AWS Athena object
athena &amp;lt;- paws::athena()

# Submit query to AWS Athena
res &amp;lt;- athena$start_query_execution(
            QueryString = &amp;quot;show Databases&amp;quot;,
            ResultConfiguration = 
                list(OutputLocation = &amp;quot;s3://mybucket/queries/&amp;quot;))

# Get Status of query
result &amp;lt;- athena$get_query_execution(QueryExecutionId = res$QueryExecutionId)

# Return results if query is successful
if(result$QueryExecution$Status$State == &amp;quot;FAILED&amp;quot;) {
  stop(result$QueryExecution$Status$StateChangeReason, call. = FALSE)
} else {output &amp;lt;- 
          athena$get_query_results(
              QueryExecutionId = res$QueryExecutionId,
              MaxResults = 1)}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From an initial view it might look daunting however this is exactly the same interface that &lt;code&gt;boto3&lt;/code&gt; provides when working with AWS Athena. The good news is that &lt;code&gt;noctua&lt;/code&gt; wraps all of this and creates the DBI method &lt;code&gt;dbGetQuery&lt;/code&gt; for &lt;code&gt;paws&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;paws&lt;/code&gt; is an excellent R SDK into AWS, so please download &lt;code&gt;paws&lt;/code&gt; and give it ago, I am sure you will be pleasantly surprised like myself.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;paws&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For more examples, the developers of &lt;code&gt;paws&lt;/code&gt; have created some code examples &lt;a href=&#34;https://github.com/paws-r/paws/tree/master/examples&#34;&gt;https://github.com/paws-r/paws/tree/master/examples&lt;/a&gt; and a  documentation website &lt;a href=&#34;https://paws-r.github.io/&#34;&gt;https://paws-r.github.io/&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;botor-https-daroczig-github-io-botor&#34;&gt;&lt;a href=&#34;https://daroczig.github.io/botor/&#34;&gt;&lt;code&gt;botor&lt;/code&gt;&lt;/a&gt; :&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;This R package provides raw access to the ‘Amazon Web Services’ (‘AWS’) ‘SDK’ via the ‘boto3’ Python module and some convenient helper functions (currently for S3 and KMS) and workarounds, eg taking care of spawning new resources in forked R processes. (&lt;a href=&#34;https://daroczig.github.io/botor/&#34;&gt;https://daroczig.github.io/botor/&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;When using &lt;code&gt;botor&lt;/code&gt; on AWS Sagemaker, R users can easily interact with all of AWS products in the exact same manner as a Python user. However &lt;code&gt;botor&lt;/code&gt;&amp;rsquo;s convenient helper functions certainly does make the experience working on AWS Sagemaker easier. Here is a quick example to demostrate how easy/ useful these helper function are:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Upload iris data.frame to s3 bucket&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(botor)

write_s3(iris, data.table::fwrite, &amp;quot;s3://mybucket/iris.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Read s3 file back into R as a data.frame&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;read_s3(&amp;quot;s3:://mybucket/iris.csv&amp;quot;, data.table::fread)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These convenient helper functions are not limited to just reading/writing data in csv format. They can also be used to upload R models, which can be really useful when wanted to store pre-built models. Here is a quick example of what I like to call a &lt;em&gt;crap&lt;/em&gt; model.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train &amp;lt;- iris[1:20,1:4]
test &amp;lt;- iris[21:40,1:4]
 
model &amp;lt;- lm(Petal.Width ~., train)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Uploading and downloading R models to S3&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;s3_write(model, saveRDS, &amp;quot;s3://mybucket/crap_model.RDS&amp;quot;)
s3_model &amp;lt;- s3_read(&amp;quot;s3://mybucket/crap_model.RDS&amp;quot;, readRDS)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It is clear to see how useful &lt;code&gt;botor&lt;/code&gt; is when working with AWS S3.&lt;/p&gt;

&lt;h2 id=&#34;cloudyr-project&#34;&gt;Cloudyr Project:&lt;/h2&gt;

&lt;p&gt;I personally haven&amp;rsquo;t used the AWS cloudyr packages, however I don&amp;rsquo;t want to leave them out. The &lt;a href=&#34;https://cloudyr.github.io/&#34;&gt;cloudyr project&lt;/a&gt; aim is to bring R onto the cloud compute:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The goal of this initiative is to make cloud computing with R easier, starting with robust tools for working with cloud computing platforms.(&lt;a href=&#34;https://cloudyr.github.io/&#34;&gt;https://cloudyr.github.io/&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As I haven&amp;rsquo;t utilised the wide range of packages that the &lt;code&gt;cloudyr project&lt;/code&gt; provides I won&amp;rsquo;t give examples. Please go to the cloudyr github &lt;a href=&#34;https://github.com/cloudyr&#34;&gt;https://github.com/cloudyr&lt;/a&gt; as a lot of work has gone into making R easier to work with cloud computing. They have a lot of documentation plus they are actively developing R packages to make user experience better.&lt;/p&gt;

&lt;h1 id=&#34;summary&#34;&gt;Summary:&lt;/h1&gt;

&lt;p&gt;I believe that all of these packages have advantages in working with AWS when using R. As R has a SDK &lt;code&gt;paws&lt;/code&gt; for AWS it would be great if it was added to the base image, as it allows R developers to utilise AWS products in their AWS Sagemaker environments. Alternatively the &lt;code&gt;botor&lt;/code&gt; package would be another package for AWS to consider putting in their AWS Sagemaker image.&lt;/p&gt;
</description>
      
                  <category>R</category>
      
                  <category>paws</category>
      
                  <category>Boto3</category>
      
                  <category>Python</category>
      
      
            <category>RBloggers</category>
      
    </item>
    
  </channel>
</rss>